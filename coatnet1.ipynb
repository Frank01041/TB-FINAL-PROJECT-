{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Reload the function by Haolun huang and split the data by Chenghan Lu, Loading the data by Zekai Zhang"
      ],
      "metadata": {
        "id": "dHUN8c2uIVQK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJYzHPi1oghc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqMTXt3P5ewQ",
        "outputId": "8be40eb8-ec84-48a3-fec5-8ee73fb86a20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mafh-y9Rog-8",
        "outputId": "72c7e9bb-f753-4ee6-d45c-c95316cf6aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-cv-attention-models\n",
            "  Downloading keras_cv_attention_models-1.3.14-py3-none-any.whl (635 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.8/635.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from keras-cv-attention-models) (8.4.0)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from keras-cv-attention-models) (2.12.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.9/dist-packages (from keras-cv-attention-models) (4.8.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (2.12.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (0.4.8)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (16.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (4.5.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (23.1)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (2.12.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (3.20.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (23.3.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (1.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (1.53.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (2.12.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (1.16.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (3.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (2.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (0.32.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (0.2.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow->keras-cv-attention-models) (0.4.0)\n",
            "Collecting typeguard<3.0.0,>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (8.1.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (2.27.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (4.65.0)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (1.2.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (0.1.8)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (5.9.5)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (2.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from tensorflow-datasets->keras-cv-attention-models) (0.10.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow->keras-cv-attention-models) (0.40.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.9/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras-cv-attention-models) (3.15.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.9/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->keras-cv-attention-models) (5.12.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow->keras-cv-attention-models) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow->keras-cv-attention-models) (1.10.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv-attention-models) (2.0.12)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (3.4.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (2.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (0.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv-attention-models) (1.59.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (6.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow->keras-cv-attention-models) (3.2.2)\n",
            "Installing collected packages: typeguard, tensorflow-addons, keras-cv-attention-models\n",
            "Successfully installed keras-cv-attention-models-1.3.14 tensorflow-addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install -U keras-cv-attention-models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oam7tMFXtdAt",
        "outputId": "4c09b4b8-317b-4dec-f5ae-0ba64343c800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#import google drive into the colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7wT3snE0yX0"
      },
      "outputs": [],
      "source": [
        "#All the file location in the share google drive\n",
        "\n",
        "filepath = '/content/drive/MyDrive/CSCI3397_Project/TBX11K' #gernal file pass\n",
        "filepath_anno = filepath + '/annotations/json/all_trainval.json' #all the annotation in Jason file, include all the image in the TBX11K\n",
        "filepath_img_train = filepath + '/imgs/train' #all the training Image (inlcude three category: health, sick, TB)\n",
        "filepath_img_test = filepath + '/imgs/test' #all the test images (all display unknown and jason file have the correct label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VpLl6ZH1eIF",
        "outputId": "76eba48b-1336-41ea-9d13-f7bda41e2ac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 1, 'file_name': 'tb/tb0003.png', 'width': 512, 'height': 512, 'date_captured': '2020-06-24 12:42:50.361310', 'license': 1, 'coco_url': '', 'flickr_url': ''}\n"
          ]
        }
      ],
      "source": [
        "#Exame all the annotations\n",
        "import json\n",
        "with open(filepath_anno, 'r') as f:\n",
        "  annotations = json.load(f)\n",
        "print(annotations['images'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlO9jIT4FLFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9485cf80-36b8-49c0-fd4c-4de40067342d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 1, 'file_name': 'tb/tb0003.png', 'width': 512, 'height': 512, 'date_captured': '2020-06-24 12:42:50.361310', 'license': 1, 'coco_url': '', 'flickr_url': ''}\n"
          ]
        }
      ],
      "source": [
        "print(annotations['images'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFfZwL1EOKOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31501cbf-31fb-471b-f68d-d91bb3257f20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8400 files belonging to 3 classes.\n",
            "Using 6720 files for training.\n",
            "Found 8400 files belonging to 3 classes.\n",
            "Using 1680 files for validation.\n"
          ]
        }
      ],
      "source": [
        "#TF split the train and validation data set from the all train \n",
        "\n",
        "\n",
        "train = tf.keras.utils.image_dataset_from_directory(\n",
        "   filepath_img_train,\n",
        "   validation_split=0.2,\n",
        "   subset = 'training',\n",
        "   seed=42, \n",
        "   image_size=(224, 224), #transfer the imgae size to the model required 224x224, instead of orginal 512x512\n",
        "   batch_size=64)\n",
        "val = tf.keras.utils.image_dataset_from_directory(\n",
        "   filepath_img_train,\n",
        "   validation_split=0.2,\n",
        "   subset = 'validation',\n",
        "   seed=42, \n",
        "   image_size=(224, 224),\n",
        "   batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--tsosHjRiGD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8e44f8c-ff3e-4c9a-a8b4-d5d5a927f05c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train #train had been change "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfhnLBfvtoLl"
      },
      "source": [
        "# Code from the paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYF5NHgZt9uG"
      },
      "source": [
        "## Pre-load the function By Zekai Zhang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka0OEqhxtz7O"
      },
      "outputs": [],
      "source": [
        "# Pre-load the function need for the model \n",
        "import os\n",
        "import re\n",
        "import datetime\n",
        "import numpy as np\n",
        "from itertools import groupby\n",
        "from skimage import measure\n",
        "from PIL import Image\n",
        "from pycocotools import mask\n",
        "from math import sqrt\n",
        "\n",
        "convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
        "natrual_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVAqFOtlt7bT"
      },
      "outputs": [],
      "source": [
        "#all the pre code from the paper \n",
        "\n",
        "convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
        "natrual_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
        "\n",
        "def resize_binary_mask(array, new_size):\n",
        "    image = Image.fromarray(array.astype(np.uint8)*255)\n",
        "    image = image.resize(new_size)\n",
        "    return np.asarray(image).astype(np.bool_)\n",
        "\n",
        "def close_contour(contour):\n",
        "    if not np.array_equal(contour[0], contour[-1]):\n",
        "        contour = np.vstack((contour, contour[0]))\n",
        "    return contour\n",
        "\n",
        "def binary_mask_to_rle(binary_mask):\n",
        "    rle = {'counts': [], 'size': list(binary_mask.shape)}\n",
        "    counts = rle.get('counts')\n",
        "    for i, (value, elements) in enumerate(groupby(binary_mask.ravel(order='F'))):\n",
        "        if i == 0 and value == 1:\n",
        "                counts.append(0)\n",
        "        counts.append(len(list(elements)))\n",
        "\n",
        "    return rle\n",
        "\n",
        "def binary_mask_to_polygon(binary_mask, tolerance=0):\n",
        "    \"\"\"Converts a binary mask to COCO polygon representation\n",
        "\n",
        "    Args:\n",
        "        binary_mask: a 2D binary numpy array where '1's represent the object\n",
        "        tolerance: Maximum distance from original points of polygon to approximated\n",
        "            polygonal chain. If tolerance is 0, the original coordinate array is returned.\n",
        "\n",
        "    \"\"\"\n",
        "    polygons = []\n",
        "    # pad mask to close contours of shapes which start and end at an edge\n",
        "    padded_binary_mask = np.pad(binary_mask, pad_width=1, mode='constant', constant_values=0)\n",
        "    contours = measure.find_contours(padded_binary_mask, 0.5)\n",
        "    contours = np.subtract(contours, 1)\n",
        "    for contour in contours:\n",
        "        contour = close_contour(contour)\n",
        "        contour = measure.approximate_polygon(contour, tolerance)\n",
        "        if len(contour) < 3:\n",
        "            continue\n",
        "        contour = np.flip(contour, axis=1)\n",
        "        segmentation = contour.ravel().tolist()\n",
        "        # after padding and subtracting 1 we may get -0.5 points in our segmentation\n",
        "        segmentation = [0 if i < 0 else i for i in segmentation]\n",
        "        polygons.append(segmentation)\n",
        "\n",
        "    return polygons\n",
        "\n",
        "def create_image_info(image_id, file_name, image_size,\n",
        "                      date_captured=datetime.datetime.utcnow().isoformat(' '),\n",
        "                      license_id=1, coco_url=\"\", flickr_url=\"\"):\n",
        "\n",
        "    image_info = {\n",
        "            \"id\": image_id,\n",
        "            \"file_name\": file_name,\n",
        "            \"width\": image_size[0],\n",
        "            \"height\": image_size[1],\n",
        "            \"date_captured\": date_captured,\n",
        "            \"license\": license_id,\n",
        "            \"coco_url\": coco_url,\n",
        "            \"flickr_url\": flickr_url\n",
        "    }\n",
        "\n",
        "    return image_info\n",
        "\n",
        "def create_annotation_info(annotation_id, image_id, category_info, binary_mask=None,\n",
        "                           image_size=None, tolerance=2, bounding_box=None):\n",
        "    assert(binary_mask is not None or image_size is not None)\n",
        "\n",
        "    if binary_mask is not None:\n",
        "        if image_size is not None:\n",
        "            binary_mask = resize_binary_mask(binary_mask, image_size)\n",
        "        binary_mask_encoded = mask.encode(np.asfortranarray(binary_mask.astype(np.uint8)))\n",
        "        area = mask.area(binary_mask_encoded)\n",
        "        bounding_box = mask.toBbox(binary_mask_encoded)\n",
        "    else:\n",
        "        area = np.array(bounding_box[2] * bounding_box[3], dtype=int)\n",
        "    if area < 20:\n",
        "        print(\"Area of this annotation is less than 20, Skip it! image_id:\", image_id, \"area:\", area, \"bbox:\", bounding_box)\n",
        "        return None\n",
        "    if category_info[\"is_crowd\"]:\n",
        "        is_crowd = 1\n",
        "        segmentation = binary_mask_to_rle(binary_mask)\n",
        "    else :\n",
        "        is_crowd = 0\n",
        "        if binary_mask is not None:\n",
        "            binary_mask_encoded = mask.encode\n",
        "            segmentation = binary_mask_to_polygon(binary_mask, tolerance)\n",
        "            if not segmentation:\n",
        "                return None\n",
        "    if binary_mask is not None:\n",
        "        annotation_info = {\n",
        "            \"id\": annotation_id,\n",
        "            \"image_id\": image_id,\n",
        "            \"category_id\": category_info[\"id\"],\n",
        "            \"iscrowd\": is_crowd,\n",
        "            \"area\": area.tolist(),\n",
        "            \"bbox\": bounding_box.tolist(),\n",
        "            \"segmentation\": segmentation,\n",
        "            \"width\": binary_mask.shape[1],\n",
        "            \"height\": binary_mask.shape[0],\n",
        "        }\n",
        "    else:\n",
        "        annotation_info = {\n",
        "            \"id\": annotation_id,\n",
        "            \"image_id\": image_id,\n",
        "            \"category_id\": category_info[\"id\"],\n",
        "            \"iscrowd\": is_crowd,\n",
        "            \"area\": area.tolist(),\n",
        "            \"bbox\": bounding_box.tolist(),\n",
        "            \"width\": image_size[0],\n",
        "            \"height\": image_size[1],\n",
        "        }\n",
        "\n",
        "    return annotation_info\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWkiIMZFonVN"
      },
      "outputs": [],
      "source": [
        "from keras_cv_attention_models import coatnet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CoAtNet Model by Zekai Zhang"
      ],
      "metadata": {
        "id": "GAX3D7juIpBx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysIbiUBrpJeS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14a6ebf9-5117-4bd9-e946-5defb73ea24c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/leondgarse/keras_cv_attention_models/releases/download/coatnet/coatnet0_224_imagenet.h5\n",
            "93747824/93747824 [==============================] - 5s 0us/step\n",
            ">>>> Load pretrained from: /root/.keras/models/coatnet0_224_imagenet.h5\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "35363/35363 [==============================] - 0s 0us/step\n",
            "[('n02124075', 'Egyptian_cat', 0.9932486), ('n02123159', 'tiger_cat', 0.0038179806), ('n02123045', 'tabby', 0.0027284513), ('n02127052', 'lynx', 5.3533517e-05), ('n04209133', 'shower_cap', 1.1487229e-05)]\n"
          ]
        }
      ],
      "source": [
        "# Only CoAtNet0 pretrained available.\n",
        "mm = coatnet.CoAtNet0()\n",
        "\n",
        "# Run prediction\n",
        "from skimage.data import chelsea\n",
        "imm = tf.keras.applications.imagenet_utils.preprocess_input(chelsea(), mode='torch') # Chelsea the cat\n",
        "pred = mm(tf.expand_dims(tf.image.resize(imm, mm.input_shape[1:3]), 0)).numpy()\n",
        "print(tf.keras.applications.imagenet_utils.decode_predictions(pred)[0])\n",
        "# [('n02124075', 'Egyptian_cat', 0.99324566), ('n02123159', 'tiger_cat', 0.00381939), ... ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Btv2acLcqFa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38564264-ed17-4829-bfcb-fd65e97c658e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " coatnet0 (Functional)       (None, 1000)              23291218  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1000)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 3003      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,294,221\n",
            "Trainable params: 23,282,573\n",
            "Non-trainable params: 11,648\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=[224, 224, 3]),\n",
        "    mm,\n",
        "    tf.keras.layers.Dropout(rate=0.4),\n",
        "    tf.keras.layers.Dense(3, activation='softmax'),\n",
        "])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = coatnet.CoAtNet0(input_shape=[224, 224, 3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzLeuyu_iJty",
        "outputId": "982adbd4-5dc8-403b-eb45-53e921160782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>>> Load pretrained from: /root/.keras/models/coatnet0_224_imagenet.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Flq4305rjRJ"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD()\n",
        "\n",
        "loss=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "metrics = tf.keras.metrics.SparseCategoricalAccuracy(name='sparse_categorical_accuracy')\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss,\n",
        "              metrics=metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fulxEb_trqv7"
      },
      "outputs": [],
      "source": [
        "#history = model.fit(train,\n",
        "                    #epochs = 2, \n",
        "                    #batch_size = 64, \n",
        "                    #validation_data = val)\n",
        "# Model wiothout Pre train is not working due to limited RAM of Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The transfer learning Code is by All Three members of the team (Chenghan Lu Haolun Huang and Zekai Zhang)"
      ],
      "metadata": {
        "id": "o4cAjTj7I8ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Alternative \n",
        "\n",
        "#Transfer learining \n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Freeze all layers except the last one\n",
        "for layer in mm.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Replace last layer with a new output layer\n",
        "num_classes = 10\n",
        "mm.layers[-1] = Dense(num_classes, activation='softmax')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kO1fD76b4wUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers except the last one\n",
        "for layer in mm.layers[:-1]:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "lEivPJQ5v-FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=[224, 224, 3]),\n",
        "    mm,\n",
        "    tf.keras.layers.Dropout(rate=0.4),\n",
        "    tf.keras.layers.Dense(3, activation='softmax'),\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yq2yL66Q7WBA",
        "outputId": "ade30476-1616-446e-9478-5584d9438e97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " coatnet0 (Functional)       (None, 1000)              23291218  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1000)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 3003      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,294,221\n",
            "Trainable params: 772,003\n",
            "Non-trainable params: 22,522,218\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.SGD()\n",
        "\n",
        "loss=tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "metrics = tf.keras.metrics.SparseCategoricalAccuracy(name='sparse_categorical_accuracy')\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss,\n",
        "              metrics=metrics)"
      ],
      "metadata": {
        "id": "_X5OrQtJ7WLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train,\n",
        "                    epochs = 2, \n",
        "                    batch_size = 64, \n",
        "                    validation_data = val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSAStdDU9OXO",
        "outputId": "75222be9-72d2-41db-c5fe-b9f8e6cfcdff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "105/105 [==============================] - 3888s 35s/step - loss: 1.0373 - sparse_categorical_accuracy: 0.4536 - val_loss: 0.9981 - val_sparse_categorical_accuracy: 0.4518\n",
            "Epoch 2/2\n",
            "105/105 [==============================] - 79s 707ms/step - loss: 0.9881 - sparse_categorical_accuracy: 0.4478 - val_loss: 0.9683 - val_sparse_categorical_accuracy: 0.4518\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}